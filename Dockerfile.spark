FROM openjdk:11-jre-slim

# Install Python 3.9 and other dependencies
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-dev \
    python3.9-distutils \
    wget \
    curl \
    && ln -s /usr/bin/python3.9 /usr/bin/python \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install pip for Python 3.9
RUN curl https://bootstrap.pypa.io/get-pip.py | python

# Set Spark/Hadoop Versions
ENV SPARK_VERSION=3.5.3
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV JAVA_HOME=/usr/local/openjdk-11
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# Download and Install Spark
RUN wget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar xvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} $SPARK_HOME && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Install the matching PySpark version
RUN pip install pyspark==${SPARK_VERSION}

# Copy scripts to start master/worker
COPY start-master.sh /start-master.sh
COPY start-worker.sh /start-worker.sh
RUN chmod +x /start-master.sh /start-worker.sh

# Expose Spark master and worker ports
# Master Web UI: 8080
# Master Port: 7077
# Worker Web UI: 8081
EXPOSE 7077 8080 8081

# By default, start the master. The worker container will override this in docker-compose.
CMD ["/start-master.sh"]
